{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load datasets from google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1642,"status":"ok","timestamp":1682713464613,"user":{"displayName":"Xinyue Li","userId":"07374751592849763732"},"user_tz":240},"id":"NlbPb6H4nXmP"},"outputs":[],"source":["import json\n","import jieba\n","import re\n","import codecs\n","import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","import sklearn.metrics as metrics\n","import matplotlib.pyplot as plt\n","from collections import Counter"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682713464613,"user":{"displayName":"Xinyue Li","userId":"07374751592849763732"},"user_tz":240},"id":"okB5a5HinYrX"},"outputs":[],"source":["# Load cleand and simplified data\n","dataset_dir = \"/content/drive/MyDrive/NLP_Final_Project/\"\n","def load_data():\n","  json_files = [dataset_dir + \"datasets/simplify_json/train_split.json\", \n","                dataset_dir + \"datasets/simplify_json/valid_split.json\", \n","                dataset_dir + \"datasets/simplify_json/test_split.json\"]\n","  data_files = []\n","  for json_file in json_files:\n","      with open(json_file, 'r') as f:\n","          data_files.append(json.load(f))\n","\n","  return data_files"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682713464613,"user":{"displayName":"Xinyue Li","userId":"07374751592849763732"},"user_tz":240},"id":"CJmokAaMnZ3I"},"outputs":[],"source":["# covert big-5 ['Openness', 'Conscientiousness', 'Extraversion','Agreeableness', 'Neuroticism'] to 0:low 1:high\n","def one_hot_encode_big5(input_data):\n","  for i in range(len(input_data)):\n","    for j in range(len(input_data[i])):\n","        input_data[i][j]['big-5'] = [0 if input_data[i][j]['Openness'] == 'low' else 1,\n","                                     0 if input_data[i][j]['Conscientiousness'] == 'low' else 1,\n","                                     0 if input_data[i][j]['Extraversion'] == 'low' else 1,\n","                                     0 if input_data[i][j]['Agreeableness'] == 'low' else 1,\n","                                     0 if input_data[i][j]['Neuroticism'] == 'low' else 1]\n","        del input_data[i][j]['Openness']\n","        del input_data[i][j]['Conscientiousness']\n","        del input_data[i][j]['Extraversion']\n","        del input_data[i][j]['Agreeableness']\n","        del input_data[i][j]['Neuroticism']\n","    converted_data = input_data\n","  return converted_data"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":125,"status":"ok","timestamp":1682713464736,"user":{"displayName":"Xinyue Li","userId":"07374751592849763732"},"user_tz":240},"id":"nvyKPXwKnbNg"},"outputs":[],"source":["# Get (X_train y_train), (X_valid, y_valid) from converted data\n","def get_X_y(converted_data):\n","  X_train = []   # name, Utterance\n","  y_train = []   # big-5\n","  X_valid = []    # name, Utterance\n","  y_valid = []    # big-5\n","  X_test = []   # name, Utterance\n","  y_test = []   # big-5\n","  speakers_train, speakers_valid, speakers_test= [],[],[]\n","\n","  for i in range(len(converted_data[0])):\n","    X_train.append((converted_data[0][i]['Speaker'], converted_data[0][i]['Utterance']))\n","    y_train.append(converted_data[0][i]['big-5'])\n","    speakers_train.append(converted_data[0][i]['Speaker'])\n","  speakers_train = list(set(speakers_train))\n","  print(\"There are {} speakers in train_data\" .format(len(speakers_train)))\n","\n","  for i in range(len(converted_data[1])):\n","    X_valid.append((converted_data[1][i]['Speaker'], converted_data[1][i]['Utterance']))\n","    y_valid.append(converted_data[1][i]['big-5'])\n","    speakers_valid.append(converted_data[1][i]['Speaker'])\n","  speakers_valid = list(set(speakers_valid))\n","  print(\"There are {} speakers in valid_data\" .format(len(speakers_valid)))\n","\n","  for i in range(len(converted_data[2])):\n","    X_test.append((converted_data[2][i]['Speaker'], converted_data[2][i]['Utterance']))\n","    y_test.append(converted_data[2][i]['big-5'])\n","    speakers_test.append(converted_data[2][i]['Speaker'])\n","  speakers_test = list(set(speakers_test))\n","  print(\"There are {} speakers in test_data\" .format(len(speakers_test)))\n","\n","  return X_train, y_train, X_valid, y_valid, X_test, y_test, speakers_train, speakers_valid, speakers_test"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1682713464736,"user":{"displayName":"Xinyue Li","userId":"07374751592849763732"},"user_tz":240},"id":"BiriAncKncjw"},"outputs":[],"source":["# combine same speaker data to one data and only keep different Utterance and one speaker name\n","def combine_same_speaker_data(speakers, X, y):  \n","    X_combine = []\n","    y_combine = []\n","    for speaker in speakers:\n","        X_combine.append({speaker:[]})\n","        y_combine.append({speaker:[]})\n","\n","    for i in range(len(X)):\n","        for j in range(len(X_combine)):\n","            if X[i][0] in X_combine[j]:\n","                X_combine[j][X[i][0]].append(X[i][1])\n","                y_combine[j][X[i][0]].append(y[i])\n","\n","    for i in range(len(y_combine)):\n","        for key in y_combine[i]:\n","            y_combine[i][key] = y_combine[i][key][0]\n","\n","    \n","    return X_combine, y_combine"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1793,"status":"ok","timestamp":1682713466527,"user":{"displayName":"Xinyue Li","userId":"07374751592849763732"},"user_tz":240},"id":"cYFFIJ4Zndxo","outputId":"e3e035cc-bede-4568-9f29-dde5e0b1d955"},"outputs":[{"data":{"text/plain":["{'Speaker': '童文洁', 'Utterance': '真巧', 'big-5': [0, 1, 1, 0, 1]}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["data_files = load_data()\n","data_files[0][0]\n","converted_data = one_hot_encode_big5(data_files)\n","converted_data[0][0]"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":179,"status":"ok","timestamp":1682713466794,"user":{"displayName":"Xinyue Li","userId":"07374751592849763732"},"user_tz":240},"id":"QVbb5Ni7neop","outputId":"fa3e7cc3-8290-491d-e007-339ea334f318"},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 220 speakers in train_data\n","There are 33 speakers in valid_data\n","There are 32 speakers in test_data\n"]}],"source":["X_train, y_train, X_valid, y_valid, X_test, y_test, speakers_train, speakers_valid, speakers_test = get_X_y(converted_data)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":1227,"status":"ok","timestamp":1682713468020,"user":{"displayName":"Xinyue Li","userId":"07374751592849763732"},"user_tz":240},"id":"patpura-nfhg"},"outputs":[],"source":["X_train_combined, y_train_combined = combine_same_speaker_data(speakers_train, X_train, y_train)\n","X_valid_combined, y_valid_combined = combine_same_speaker_data(speakers_valid, X_valid, y_valid)\n","X_test_combined, y_test_combined = combine_same_speaker_data(speakers_test, X_test, y_test)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682713468020,"user":{"displayName":"Xinyue Li","userId":"07374751592849763732"},"user_tz":240},"id":"G47b2dNGng1Y"},"outputs":[],"source":["# Use jieba tokenizer\n","def tokenize_data(speaker, X):\n","  X_train_tokenized = X\n","  for speaker in X_train_tokenized:\n","    for key in speaker:\n","      for i in range(len(speaker[key])):\n","          seg_list = jieba.cut(speaker[key][i], cut_all=False)\n","          speaker[key][i] = \" \".join(seg_list)\n","          if i > 0:\n","            speaker[key][0] = speaker[key][0] + \" \" +speaker[key][i]\n","    for key in speaker:\n","        for j in reversed(range(1, len(speaker[key]))):\n","            # delete other key's value\n","            del speaker[key][j]\n","    for key in speaker:\n","        # Remove non-alphanumeric characters (except underscores) and convert to lowercase\n","        speaker[key][0] = re.sub(r'[^\\w\\s_]', '', speaker[key][0])\n","        # Replace multiple whitespace characters with a single space\n","        speaker[key][0] = re.sub(r'\\r\\s+', ' ', speaker[key][0])\n","  return X_train_tokenized"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5429,"status":"ok","timestamp":1682713473446,"user":{"displayName":"Xinyue Li","userId":"07374751592849763732"},"user_tz":240},"id":"pi7raZsVniGq","outputId":"a7b6d46d-f3c6-4e6b-f5f2-954fbcd21cbf"},"outputs":[{"name":"stderr","output_type":"stream","text":["Building prefix dict from the default dictionary ...\n","Loading model from cache C:\\Users\\E48D~1\\AppData\\Local\\Temp\\jieba.cache\n","Loading model cost 0.532 seconds.\n","Prefix dict has been built successfully.\n"]}],"source":["X_train_tokenized = tokenize_data(speakers_train, X_train_combined)\n","X_valid_tokenized = tokenize_data(speakers_valid, X_valid_combined)\n","X_test_tokenized = tokenize_data(speakers_test, X_test_combined)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682713473447,"user":{"displayName":"Xinyue Li","userId":"07374751592849763732"},"user_tz":240},"id":"UxcR96Lxnjc5"},"outputs":[],"source":["# Remove speaker from X and get the value of y (O of big-5)\n","def clean_X_y(X_tokenized, y):\n","  X_cleaned = []\n","  y_cleaned = []\n","  for line in X_tokenized:\n","    for speaker in line:\n","          X_cleaned.append(line[speaker][0])\n","  for line in y:\n","    for speaker in line:\n","      # Change i in (line[speaker][i]) to get different y in OCEAN: {0:O, 1:C, 2:E, 3:A, 4:N }\n","      y_cleaned.append(line[speaker])\n","            \n","  return X_cleaned, pd.DataFrame(y_cleaned, columns=['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism'])"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682713473447,"user":{"displayName":"Xinyue Li","userId":"07374751592849763732"},"user_tz":240},"id":"I9gm2g_mnkjB"},"outputs":[],"source":["X_train_cleaned, y_train_cleaned = clean_X_y(X_train_tokenized, y_train_combined)\n","X_valid_cleaned, y_valid_cleaned = clean_X_y(X_valid_tokenized, y_valid_combined)\n","X_test_cleaned, y_test_cleaned = clean_X_y(X_test_tokenized, y_test_combined)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Openness</th>\n","      <th>Conscientiousness</th>\n","      <th>Extraversion</th>\n","      <th>Agreeableness</th>\n","      <th>Neuroticism</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>215</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>216</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>217</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>218</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>219</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>220 rows × 5 columns</p>\n","</div>"],"text/plain":["     Openness  Conscientiousness  Extraversion  Agreeableness  Neuroticism\n","0           0                  1             0              0            0\n","1           1                  1             0              1            0\n","2           0                  0             1              0            1\n","3           1                  1             0              1            1\n","4           0                  1             0              0            0\n","..        ...                ...           ...            ...          ...\n","215         1                  1             0              0            1\n","216         0                  1             0              0            1\n","217         0                  0             1              0            1\n","218         1                  1             0              1            0\n","219         0                  1             1              0            1\n","\n","[220 rows x 5 columns]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["y_train_cleaned = y_train_cleaned[['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']]\n","y_train_cleaned"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682713473447,"user":{"displayName":"Xinyue Li","userId":"07374751592849763732"},"user_tz":240},"id":"EJGWfs8tnl4x"},"outputs":[],"source":["# load stopword\n","def load_stopwords(stopwords_file):\n","  stopwords = []\n","  with codecs.open(stopwords_file, 'r', encoding='utf-8', errors='ignore') as fp:\n","      stopwords = fp.read().split('\\r\\n')\n","  return stopwords"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":554,"status":"ok","timestamp":1682713473998,"user":{"displayName":"Xinyue Li","userId":"07374751592849763732"},"user_tz":240},"id":"arDaARQfnmvC"},"outputs":[],"source":["stopwords_file     = dataset_dir + \"stopwords-master/cn_stopwords.txt\"\n","stop_words = load_stopwords(stopwords_file)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":577,"status":"ok","timestamp":1682713474573,"user":{"displayName":"Xinyue Li","userId":"07374751592849763732"},"user_tz":240},"id":"lSwytkHZnoCx","outputId":"80b181d6-b979-4ad4-acb7-a6c217f0fbe8"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\李芯悦\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n","  warnings.warn(\n"]}],"source":["vectorizer = CountVectorizer(stop_words=stop_words)\n","X_features_train = vectorizer.fit_transform(X_train_cleaned)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":4898,"status":"ok","timestamp":1682713479469,"user":{"displayName":"Xinyue Li","userId":"07374751592849763732"},"user_tz":240},"id":"LZnrF3O4na9g"},"outputs":[],"source":["from keras import Sequential, optimizers\n","from keras.layers import Dense, Dropout, LSTM, Embedding, SimpleRNN, Flatten, GRU\n","from keras.utils import pad_sequences\n","from keras.preprocessing import sequence\n","from keras.preprocessing.text import Tokenizer"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682713479469,"user":{"displayName":"Xinyue Li","userId":"07374751592849763732"},"user_tz":240},"id":"EHVA48DIv_jC"},"outputs":[],"source":["def binary_label(y):\n","  for i in range(len(y)):\n","    for j in range(5):\n","      if y[i][j] < 0.5:\n","        y[i][j] = 0\n","      else:\n","        y[i][j] = 1\n","  return y\n","\n","def labels_acc(y_true, y_pred):\n","  y_true = y_true.numpy()\n","  y_pred = y_pred.numpy()\n","  true_labels = np.argmax(y_true, axis=1)\n","  pred_labels = np.round(y_pred)\n","  accuracy = np.mean(np.all(np.equal(true_labels, pred_labels), axis=1))\n","  return accuracy"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["X_train = []\n","for i in range(len(X_train_cleaned)):\n","    sentence = X_train_cleaned[i]\n","    lst = sentence.split(' ')\n","    seq = [word for word in lst if word not in stop_words]\n","    X_train.append(' '.join(seq))\n","X_train = pd.DataFrame(X_train, columns=['words'])\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X_train['words'])\n","sequences = tokenizer.texts_to_sequences(X_train['words'])\n","max_len = 128\n","padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n","\n","X_test = []\n","for i in range(len(X_test_cleaned)):\n","    sentence = X_test_cleaned[i]\n","    lst = sentence.split(' ')\n","    seq = [word for word in lst if word not in stop_words]\n","    X_test.append(' '.join(seq))\n","X_test = pd.DataFrame(X_test, columns=['words'])\n","\n","tokenizer_test = Tokenizer()\n","tokenizer_test.fit_on_texts(X_test['words'])\n","sequences_test = tokenizer_test.texts_to_sequences(X_test['words'])\n","max_len = 128\n","padded_sequences_test = pad_sequences(sequences_test, maxlen=max_len, padding='post', truncating='post')"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"D5-jVvwroZwr"},"outputs":[],"source":["def ANN(X, y):\n","  model = Sequential()\n","\n","  model.add(Dense(256, input_dim=padded_sequences.shape[1]))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(256, activation='relu'))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(90, activation='relu'))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(5, activation='sigmoid'))\n","  \n","  model.compile(loss='binary_crossentropy', optimizer = optimizers.Adam(), metrics=['accuracy'])\n","  print(model.summary())\n","\n","  model.fit(X, y, epochs=50, batch_size=5)\n","  return model"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_8 (Dense)             (None, 256)               65792     \n","                                                                 \n"," dropout_6 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_9 (Dense)             (None, 256)               65792     \n","                                                                 \n"," dropout_7 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_10 (Dense)            (None, 90)                23130     \n","                                                                 \n"," dropout_8 (Dropout)         (None, 90)                0         \n","                                                                 \n"," dense_11 (Dense)            (None, 5)                 455       \n","                                                                 \n","=================================================================\n","Total params: 155,169\n","Trainable params: 155,169\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/50\n","44/44 [==============================] - 0s 1ms/step - loss: 3403.7012 - accuracy: 0.2273\n","Epoch 2/50\n","44/44 [==============================] - 0s 1ms/step - loss: 2415.7961 - accuracy: 0.2227\n","Epoch 3/50\n","44/44 [==============================] - 0s 1ms/step - loss: 1407.2147 - accuracy: 0.2682\n","Epoch 4/50\n","44/44 [==============================] - 0s 1ms/step - loss: 1084.4196 - accuracy: 0.2409\n","Epoch 5/50\n","44/44 [==============================] - 0s 1ms/step - loss: 751.7051 - accuracy: 0.1955\n","Epoch 6/50\n","44/44 [==============================] - 0s 1ms/step - loss: 598.0280 - accuracy: 0.2045\n","Epoch 7/50\n","44/44 [==============================] - 0s 1ms/step - loss: 279.9864 - accuracy: 0.3182\n","Epoch 8/50\n","44/44 [==============================] - 0s 1ms/step - loss: 190.9406 - accuracy: 0.3273\n","Epoch 9/50\n","44/44 [==============================] - 0s 1ms/step - loss: 111.0022 - accuracy: 0.2773\n","Epoch 10/50\n","44/44 [==============================] - 0s 1ms/step - loss: 74.6835 - accuracy: 0.3273\n","Epoch 11/50\n","44/44 [==============================] - 0s 1ms/step - loss: 83.7474 - accuracy: 0.3227\n","Epoch 12/50\n","44/44 [==============================] - 0s 1ms/step - loss: 39.0498 - accuracy: 0.3318\n","Epoch 13/50\n","44/44 [==============================] - 0s 1ms/step - loss: 57.9479 - accuracy: 0.3045\n","Epoch 14/50\n","44/44 [==============================] - 0s 1ms/step - loss: 50.4500 - accuracy: 0.3364\n","Epoch 15/50\n","44/44 [==============================] - 0s 1ms/step - loss: 23.4640 - accuracy: 0.3591\n","Epoch 16/50\n","44/44 [==============================] - 0s 1ms/step - loss: 10.6503 - accuracy: 0.3455\n","Epoch 17/50\n","44/44 [==============================] - 0s 1ms/step - loss: 22.4100 - accuracy: 0.3500\n","Epoch 18/50\n","44/44 [==============================] - 0s 1ms/step - loss: 6.4705 - accuracy: 0.3364\n","Epoch 19/50\n","44/44 [==============================] - 0s 1ms/step - loss: 10.3082 - accuracy: 0.3182\n","Epoch 20/50\n","44/44 [==============================] - 0s 1ms/step - loss: 22.3756 - accuracy: 0.3364\n","Epoch 21/50\n","44/44 [==============================] - 0s 1ms/step - loss: 16.1539 - accuracy: 0.3318\n","Epoch 22/50\n","44/44 [==============================] - 0s 1ms/step - loss: 5.3924 - accuracy: 0.3364\n","Epoch 23/50\n","44/44 [==============================] - 0s 1ms/step - loss: 15.1609 - accuracy: 0.3500\n","Epoch 24/50\n","44/44 [==============================] - 0s 1ms/step - loss: 2.9512 - accuracy: 0.3500\n","Epoch 25/50\n","44/44 [==============================] - 0s 1ms/step - loss: 6.6643 - accuracy: 0.3318\n","Epoch 26/50\n","44/44 [==============================] - 0s 1ms/step - loss: 6.5640 - accuracy: 0.3364\n","Epoch 27/50\n","44/44 [==============================] - 0s 1ms/step - loss: 8.7488 - accuracy: 0.3364  \n","Epoch 28/50\n","44/44 [==============================] - 0s 1ms/step - loss: 3.7302 - accuracy: 0.3409\n","Epoch 29/50\n","44/44 [==============================] - 0s 1ms/step - loss: 9.9633 - accuracy: 0.3409\n","Epoch 30/50\n","44/44 [==============================] - 0s 1ms/step - loss: 6.9190 - accuracy: 0.3500\n","Epoch 31/50\n","44/44 [==============================] - 0s 1ms/step - loss: 1.6218 - accuracy: 0.3364\n","Epoch 32/50\n","44/44 [==============================] - 0s 2ms/step - loss: 6.6387 - accuracy: 0.3364\n","Epoch 33/50\n","44/44 [==============================] - 0s 1ms/step - loss: 3.5293 - accuracy: 0.3455\n","Epoch 34/50\n","44/44 [==============================] - 0s 1ms/step - loss: 8.8948 - accuracy: 0.3455\n","Epoch 35/50\n","44/44 [==============================] - 0s 1ms/step - loss: 12.6536 - accuracy: 0.3500\n","Epoch 36/50\n","44/44 [==============================] - 0s 1ms/step - loss: 0.8982 - accuracy: 0.3455\n","Epoch 37/50\n","44/44 [==============================] - 0s 1ms/step - loss: 0.7272 - accuracy: 0.3455\n","Epoch 38/50\n","44/44 [==============================] - 0s 1ms/step - loss: 5.5604 - accuracy: 0.3500\n","Epoch 39/50\n","44/44 [==============================] - 0s 1ms/step - loss: 4.5108 - accuracy: 0.3500\n","Epoch 40/50\n","44/44 [==============================] - 0s 1ms/step - loss: 0.6833 - accuracy: 0.3409\n","Epoch 41/50\n","44/44 [==============================] - 0s 1ms/step - loss: 3.7523 - accuracy: 0.3409\n","Epoch 42/50\n","44/44 [==============================] - 0s 1ms/step - loss: 0.6746 - accuracy: 0.3455\n","Epoch 43/50\n","44/44 [==============================] - 0s 1ms/step - loss: 0.6354 - accuracy: 0.3455\n","Epoch 44/50\n","44/44 [==============================] - 0s 1ms/step - loss: 3.3261 - accuracy: 0.3364\n","Epoch 45/50\n","44/44 [==============================] - 0s 1ms/step - loss: 0.6402 - accuracy: 0.3455\n","Epoch 46/50\n","44/44 [==============================] - 0s 1ms/step - loss: 1.9384 - accuracy: 0.3500\n","Epoch 47/50\n","44/44 [==============================] - 0s 1ms/step - loss: 1.3194 - accuracy: 0.3455\n","Epoch 48/50\n","44/44 [==============================] - 0s 1ms/step - loss: 1.0738 - accuracy: 0.3455\n","Epoch 49/50\n","44/44 [==============================] - 0s 1ms/step - loss: 1.3955 - accuracy: 0.3409\n","Epoch 50/50\n","44/44 [==============================] - 0s 1ms/step - loss: 0.8822 - accuracy: 0.3455\n"]}],"source":["model = ANN(padded_sequences, np.array(y_train_cleaned))"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Classifying test data\n","1/1 [==============================] - 0s 50ms/step\n","Accuracy in all = 0.68125\n","Accuracy per person\n","   count\n","1      1\n","2      6\n","3     11\n","4      7\n","5      7\n","Precision for macro-label = 0.8\n","Recall    for macro-label = 0.575\n","Precision for micro-label = 0.8598130841121495\n","Recall    for micro-label = 0.71875\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\李芯悦\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["print(\"Classifying test data\")\n","predicted_labels = binary_label(model.predict(padded_sequences_test))\n","y_test_cleaned = np.array(y_test_cleaned)\n","accuracy_all = np.sum(predicted_labels == y_test_cleaned)/(len(predicted_labels)*len(predicted_labels[0]))\n","print('Accuracy in all = {}'.format(accuracy_all))\n","accuracy_person = []\n","for i in range(len(predicted_labels)):\n","    acc = np.sum(predicted_labels[i] == y_test_cleaned[i])\n","    accuracy_person.append(acc)\n","accuracy_per_person = pd.DataFrame.from_dict(Counter(accuracy_person), orient='index', columns=['count'])\n","accuracy_per_person.sort_index(inplace=True)\n","print('Accuracy per person')\n","print(accuracy_per_person)\n","print('Precision for macro-label = {}'.format(metrics.precision_score(predicted_labels, y_test_cleaned, average='macro')))\n","print('Recall    for macro-label = {}'.format(metrics.recall_score(predicted_labels,    y_test_cleaned, average='macro')))\n","print('Precision for micro-label = {}'.format(metrics.precision_score(predicted_labels, y_test_cleaned, average='micro')))\n","print('Recall    for micro-label = {}'.format(metrics.recall_score(predicted_labels,    y_test_cleaned, average='micro')))"]},{"cell_type":"code","execution_count":220,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","44/44 [==============================] - 4s 64ms/step - loss: 0.6816 - accuracy: 0.3182\n","Epoch 2/50\n","44/44 [==============================] - 3s 70ms/step - loss: 0.6445 - accuracy: 0.3318\n","Epoch 3/50\n","44/44 [==============================] - 3s 71ms/step - loss: 0.5984 - accuracy: 0.3409\n","Epoch 4/50\n","44/44 [==============================] - 3s 70ms/step - loss: 0.5221 - accuracy: 0.3727\n","Epoch 5/50\n","44/44 [==============================] - 3s 69ms/step - loss: 0.4707 - accuracy: 0.4091\n","Epoch 6/50\n","44/44 [==============================] - 3s 71ms/step - loss: 0.4163 - accuracy: 0.4227\n","Epoch 7/50\n","44/44 [==============================] - 3s 71ms/step - loss: 0.3935 - accuracy: 0.4409\n","Epoch 8/50\n","44/44 [==============================] - 3s 78ms/step - loss: 0.3638 - accuracy: 0.4000\n","Epoch 9/50\n","44/44 [==============================] - 3s 75ms/step - loss: 0.3343 - accuracy: 0.4455\n","Epoch 10/50\n","44/44 [==============================] - 3s 72ms/step - loss: 0.3045 - accuracy: 0.4727\n","Epoch 11/50\n","44/44 [==============================] - 3s 72ms/step - loss: 0.2715 - accuracy: 0.4545\n","Epoch 12/50\n","44/44 [==============================] - 3s 72ms/step - loss: 0.2540 - accuracy: 0.4545\n","Epoch 13/50\n","44/44 [==============================] - 3s 72ms/step - loss: 0.2346 - accuracy: 0.4318\n","Epoch 14/50\n","44/44 [==============================] - 3s 71ms/step - loss: 0.2111 - accuracy: 0.4318\n","Epoch 15/50\n","44/44 [==============================] - 3s 71ms/step - loss: 0.1904 - accuracy: 0.4000\n","Epoch 16/50\n","44/44 [==============================] - 3s 64ms/step - loss: 0.2248 - accuracy: 0.4000\n","Epoch 17/50\n","44/44 [==============================] - 3s 67ms/step - loss: 0.1765 - accuracy: 0.4318\n","Epoch 18/50\n","44/44 [==============================] - 3s 74ms/step - loss: 0.1577 - accuracy: 0.4227\n","Epoch 19/50\n","44/44 [==============================] - 3s 70ms/step - loss: 0.1600 - accuracy: 0.3636\n","Epoch 20/50\n","44/44 [==============================] - 3s 72ms/step - loss: 0.1441 - accuracy: 0.3591\n","Epoch 21/50\n","44/44 [==============================] - 3s 74ms/step - loss: 0.1424 - accuracy: 0.3909\n","Epoch 22/50\n","44/44 [==============================] - 3s 69ms/step - loss: 0.1465 - accuracy: 0.3682\n","Epoch 23/50\n","44/44 [==============================] - 3s 67ms/step - loss: 0.1268 - accuracy: 0.4318\n","Epoch 24/50\n","44/44 [==============================] - 3s 68ms/step - loss: 0.1178 - accuracy: 0.4000\n","Epoch 25/50\n","44/44 [==============================] - 3s 72ms/step - loss: 0.1150 - accuracy: 0.4091\n","Epoch 26/50\n","44/44 [==============================] - 3s 70ms/step - loss: 0.1110 - accuracy: 0.4136\n","Epoch 27/50\n","44/44 [==============================] - 3s 67ms/step - loss: 0.1037 - accuracy: 0.4273\n","Epoch 28/50\n","44/44 [==============================] - 3s 69ms/step - loss: 0.1090 - accuracy: 0.3773\n","Epoch 29/50\n","44/44 [==============================] - 3s 72ms/step - loss: 0.0982 - accuracy: 0.4000\n","Epoch 30/50\n","44/44 [==============================] - 3s 72ms/step - loss: 0.0961 - accuracy: 0.3864\n","Epoch 31/50\n","44/44 [==============================] - 3s 74ms/step - loss: 0.1174 - accuracy: 0.3364\n","Epoch 32/50\n","44/44 [==============================] - 3s 68ms/step - loss: 0.1051 - accuracy: 0.3409\n","Epoch 33/50\n","44/44 [==============================] - 3s 67ms/step - loss: 0.0998 - accuracy: 0.4136\n","Epoch 34/50\n","44/44 [==============================] - 3s 67ms/step - loss: 0.0890 - accuracy: 0.3636\n","Epoch 35/50\n","44/44 [==============================] - 3s 67ms/step - loss: 0.0919 - accuracy: 0.3909\n","Epoch 36/50\n","44/44 [==============================] - 3s 71ms/step - loss: 0.0954 - accuracy: 0.3318\n","Epoch 37/50\n","44/44 [==============================] - 3s 67ms/step - loss: 0.0893 - accuracy: 0.4136\n","Epoch 38/50\n","44/44 [==============================] - 3s 62ms/step - loss: 0.0938 - accuracy: 0.3682\n","Epoch 39/50\n","44/44 [==============================] - 3s 66ms/step - loss: 0.0805 - accuracy: 0.3909\n","Epoch 40/50\n","44/44 [==============================] - 3s 66ms/step - loss: 0.0831 - accuracy: 0.3955\n","Epoch 41/50\n","44/44 [==============================] - 3s 72ms/step - loss: 0.0842 - accuracy: 0.3909\n","Epoch 42/50\n","44/44 [==============================] - 3s 69ms/step - loss: 0.0754 - accuracy: 0.3636\n","Epoch 43/50\n","44/44 [==============================] - 3s 65ms/step - loss: 0.0803 - accuracy: 0.3636\n","Epoch 44/50\n","44/44 [==============================] - 3s 72ms/step - loss: 0.0821 - accuracy: 0.3773\n","Epoch 45/50\n","44/44 [==============================] - 3s 65ms/step - loss: 0.0758 - accuracy: 0.4000\n","Epoch 46/50\n","44/44 [==============================] - 3s 63ms/step - loss: 0.0721 - accuracy: 0.3682\n","Epoch 47/50\n","44/44 [==============================] - 3s 63ms/step - loss: 0.0762 - accuracy: 0.3955\n","Epoch 48/50\n","44/44 [==============================] - 3s 63ms/step - loss: 0.0724 - accuracy: 0.4182\n","Epoch 49/50\n","44/44 [==============================] - 3s 62ms/step - loss: 0.0911 - accuracy: 0.3727\n","Epoch 50/50\n","44/44 [==============================] - 3s 62ms/step - loss: 0.1289 - accuracy: 0.3455\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x289afca0e50>"]},"execution_count":220,"metadata":{},"output_type":"execute_result"}],"source":["# LSTM\n","model = Sequential()\n","\n","model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=128))\n","model.add(LSTM(32))\n","model.add(Dropout(0.5))\n","model.add(Flatten())\n","model.add(Dense(5, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n","model.fit(padded_sequences, np.array(y_train_cleaned), epochs=50, batch_size=5)"]},{"cell_type":"code","execution_count":222,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Classifying test data\n","1/1 [==============================] - 0s 27ms/step\n","Accuracy in all = 0.68125\n","Accuracy per person\n","   count\n","1      2\n","2      6\n","3     10\n","4      5\n","5      9\n","Precision for macro-label = 0.7461172161172162\n","Recall    for macro-label = 0.7322271755827156\n","Precision for micro-label = 0.7757009345794392\n","Recall    for micro-label = 0.7545454545454545\n"]}],"source":["print(\"Classifying test data\")\n","predicted_labels = binary_label(model.predict(padded_sequences_test))\n","y_test_cleaned = np.array(y_test_cleaned)\n","accuracy_all = np.sum(predicted_labels == y_test_cleaned)/(len(predicted_labels)*len(predicted_labels[0]))\n","print('Accuracy in all = {}'.format(accuracy_all))\n","accuracy_person = []\n","for i in range(len(predicted_labels)):\n","    acc = np.sum(predicted_labels[i] == y_test_cleaned[i])\n","    accuracy_person.append(acc)\n","accuracy_per_person = pd.DataFrame.from_dict(Counter(accuracy_person), orient='index', columns=['count'])\n","accuracy_per_person.sort_index(inplace=True)\n","print('Accuracy per person')\n","print(accuracy_per_person)\n","print('Precision for macro-label = {}'.format(metrics.precision_score(predicted_labels, y_test_cleaned, average='macro')))\n","print('Recall    for macro-label = {}'.format(metrics.recall_score(predicted_labels,    y_test_cleaned, average='macro')))\n","print('Precision for micro-label = {}'.format(metrics.precision_score(predicted_labels, y_test_cleaned, average='micro')))\n","print('Recall    for micro-label = {}'.format(metrics.recall_score(predicted_labels,    y_test_cleaned, average='micro')))"]},{"cell_type":"code","execution_count":249,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","44/44 [==============================] - 3s 31ms/step - loss: 0.6830 - accuracy: 0.1727\n","Epoch 2/50\n","44/44 [==============================] - 1s 33ms/step - loss: 0.6526 - accuracy: 0.1364\n","Epoch 3/50\n","44/44 [==============================] - 1s 32ms/step - loss: 0.6208 - accuracy: 0.2136\n","Epoch 4/50\n","44/44 [==============================] - 1s 33ms/step - loss: 0.5892 - accuracy: 0.2500\n","Epoch 5/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.5340 - accuracy: 0.2273\n","Epoch 6/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.5021 - accuracy: 0.2182\n","Epoch 7/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.4669 - accuracy: 0.2409\n","Epoch 8/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.4198 - accuracy: 0.2318\n","Epoch 9/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.3532 - accuracy: 0.2273\n","Epoch 10/50\n","44/44 [==============================] - 1s 32ms/step - loss: 0.3071 - accuracy: 0.2545\n","Epoch 11/50\n","44/44 [==============================] - 1s 32ms/step - loss: 0.2685 - accuracy: 0.3045\n","Epoch 12/50\n","44/44 [==============================] - 1s 32ms/step - loss: 0.2434 - accuracy: 0.3455\n","Epoch 13/50\n","44/44 [==============================] - 1s 32ms/step - loss: 0.2199 - accuracy: 0.3227\n","Epoch 14/50\n","44/44 [==============================] - 1s 34ms/step - loss: 0.2029 - accuracy: 0.3409\n","Epoch 15/50\n","44/44 [==============================] - 1s 32ms/step - loss: 0.1932 - accuracy: 0.3682\n","Epoch 16/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.1714 - accuracy: 0.3591\n","Epoch 17/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.1793 - accuracy: 0.3136\n","Epoch 18/50\n","44/44 [==============================] - 2s 34ms/step - loss: 0.1704 - accuracy: 0.3682\n","Epoch 19/50\n","44/44 [==============================] - 1s 32ms/step - loss: 0.1736 - accuracy: 0.3727\n","Epoch 20/50\n","44/44 [==============================] - 1s 32ms/step - loss: 0.1483 - accuracy: 0.3636\n","Epoch 21/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.1493 - accuracy: 0.3636\n","Epoch 22/50\n","44/44 [==============================] - 1s 32ms/step - loss: 0.1428 - accuracy: 0.3273\n","Epoch 23/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.1363 - accuracy: 0.3955\n","Epoch 24/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.1357 - accuracy: 0.3727\n","Epoch 25/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.1253 - accuracy: 0.3864\n","Epoch 26/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.1247 - accuracy: 0.3227\n","Epoch 27/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.1199 - accuracy: 0.4091\n","Epoch 28/50\n","44/44 [==============================] - 1s 33ms/step - loss: 0.1224 - accuracy: 0.4000\n","Epoch 29/50\n","44/44 [==============================] - 1s 33ms/step - loss: 0.1122 - accuracy: 0.4136\n","Epoch 30/50\n","44/44 [==============================] - 1s 33ms/step - loss: 0.1103 - accuracy: 0.3773\n","Epoch 31/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.1276 - accuracy: 0.4045\n","Epoch 32/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.1181 - accuracy: 0.3909\n","Epoch 33/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.1097 - accuracy: 0.3500\n","Epoch 34/50\n","44/44 [==============================] - 1s 32ms/step - loss: 0.1068 - accuracy: 0.3909\n","Epoch 35/50\n","44/44 [==============================] - 1s 32ms/step - loss: 0.1022 - accuracy: 0.4000\n","Epoch 36/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.0981 - accuracy: 0.3864\n","Epoch 37/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.1019 - accuracy: 0.3636\n","Epoch 38/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.0976 - accuracy: 0.3500\n","Epoch 39/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.1082 - accuracy: 0.3864\n","Epoch 40/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.0909 - accuracy: 0.4045\n","Epoch 41/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.0866 - accuracy: 0.4318\n","Epoch 42/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.0947 - accuracy: 0.3364\n","Epoch 43/50\n","44/44 [==============================] - 1s 33ms/step - loss: 0.0980 - accuracy: 0.4000\n","Epoch 44/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.0904 - accuracy: 0.3500\n","Epoch 45/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.0824 - accuracy: 0.4091\n","Epoch 46/50\n","44/44 [==============================] - 1s 33ms/step - loss: 0.0928 - accuracy: 0.3091\n","Epoch 47/50\n","44/44 [==============================] - 2s 34ms/step - loss: 0.0857 - accuracy: 0.3727\n","Epoch 48/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.0788 - accuracy: 0.3955\n","Epoch 49/50\n","44/44 [==============================] - 1s 30ms/step - loss: 0.0855 - accuracy: 0.4364\n","Epoch 50/50\n","44/44 [==============================] - 1s 31ms/step - loss: 0.0790 - accuracy: 0.3818\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x289bcb31c60>"]},"execution_count":249,"metadata":{},"output_type":"execute_result"}],"source":["# GRU\n","model = Sequential()\n","\n","model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=64))\n","model.add(GRU(32))\n","model.add(Dropout(0.5))\n","model.add(Flatten())\n","model.add(Dense(5, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n","model.fit(padded_sequences, np.array(y_train_cleaned), epochs=50, batch_size=5)"]},{"cell_type":"code","execution_count":250,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Classifying test data\n","1/1 [==============================] - 0s 308ms/step\n","Accuracy in all = 0.66875\n","Accuracy per person\n","   count\n","1      2\n","2      4\n","3     11\n","4     11\n","5      4\n","Precision for macro-label = 0.7449670329670328\n","Recall    for macro-label = 0.7153912753912753\n","Precision for micro-label = 0.7757009345794392\n","Recall    for micro-label = 0.7410714285714286\n"]}],"source":["print(\"Classifying test data\")\n","predicted_labels = binary_label(model.predict(padded_sequences_test))\n","y_test_cleaned = np.array(y_test_cleaned)\n","accuracy_all = np.sum(predicted_labels == y_test_cleaned)/(len(predicted_labels)*len(predicted_labels[0]))\n","print('Accuracy in all = {}'.format(accuracy_all))\n","accuracy_person = []\n","for i in range(len(predicted_labels)):\n","    acc = np.sum(predicted_labels[i] == y_test_cleaned[i])\n","    accuracy_person.append(acc)\n","accuracy_per_person = pd.DataFrame.from_dict(Counter(accuracy_person), orient='index', columns=['count'])\n","accuracy_per_person.sort_index(inplace=True)\n","print('Accuracy per person')\n","print(accuracy_per_person)\n","print('Precision for macro-label = {}'.format(metrics.precision_score(predicted_labels, y_test_cleaned, average='macro')))\n","print('Recall    for macro-label = {}'.format(metrics.recall_score(predicted_labels,    y_test_cleaned, average='macro')))\n","print('Precision for micro-label = {}'.format(metrics.precision_score(predicted_labels, y_test_cleaned, average='micro')))\n","print('Recall    for micro-label = {}'.format(metrics.recall_score(predicted_labels,    y_test_cleaned, average='micro')))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
