# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZjUvn_Yt8MDzaUq1NuxpLdc-5SUHFHdT
"""

# Load datasets from google drive
from google.colab import drive
drive.mount('/content/drive')

import json
import jieba
import re
import codecs
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
import sklearn.metrics as metrics
import matplotlib.pyplot as plt

# Load cleand and simplified data
dataset_dir = "/content/drive/MyDrive/NLP_Final_Project"
def load_data():
  json_files = [dataset_dir + "/datasets/simplify_json/train_split.json", dataset_dir + "/datasets/simplify_json/valid_split.json"]
  data_files = []
  for json_file in json_files:
      with open(json_file, 'r') as f:
          data_files.append(json.load(f))

  return data_files

# covert big-5 ['Openness', 'Conscientiousness', 'Extraversion','Agreeableness', 'Neuroticism'] to 0:low 1:high
def one_hot_encode_big5(input_data):
  for i in range(len(input_data)):
    for j in range(len(input_data[i])):
        input_data[i][j]['big-5'] = [0 if input_data[i][j]['Openness'] == 'low' else 1,
                                     0 if input_data[i][j]['Conscientiousness'] == 'low' else 1,
                                     0 if input_data[i][j]['Extraversion'] == 'low' else 1,
                                     0 if input_data[i][j]['Agreeableness'] == 'low' else 1,
                                     0 if input_data[i][j]['Neuroticism'] == 'low' else 1]
        del input_data[i][j]['Openness']
        del input_data[i][j]['Conscientiousness']
        del input_data[i][j]['Extraversion']
        del input_data[i][j]['Agreeableness']
        del input_data[i][j]['Neuroticism']
    converted_data = input_data
  return converted_data

# Get (X_train y_train), (X_valid, y_valid) from converted data
def get_X_y(converted_data):
  X_train = []   # name, Utterance
  y_train = []   # big-5
  X_valid = []    # name, Utterance
  y_valid = []    # big-5
  speakers_train, speakers_valid = [],[]
  for i in range(len(converted_data[0])):
    X_train.append(converted_data[0][i]['Utterance'])
    y_train.append(converted_data[0][i]['big-5'])
  print("There are {} data in train_data" .format(len(X_train)))

  for i in range(len(converted_data[1])):
    X_valid.append(converted_data[1][i]['Utterance'])
    y_valid.append(converted_data[1][i]['big-5'])
  print("There are {} data in valid_data" .format(len(X_valid)))
  return X_train, y_train, X_valid, y_valid

data_files = load_data()
data_files[0][0]
converted_data = one_hot_encode_big5(data_files)
converted_data[0][0]

X_train, y_train, X_valid, y_valid = get_X_y(converted_data)

# Use jieba tokenizer
def tokenize_data(X):
  X_train_tokenized = X
  for i in range(len(X)):
      seg_list = jieba.cut(X[i], cut_all=False)
      X[i] = " ".join(seg_list)
      X[i] = re.sub(r'[^\w\s_]','', X[i])
      X[i] = re.sub(r'\s+', ' ', X[i])
  return X_train_tokenized

X_train_tokenized = tokenize_data(X_train)
X_valid_tokenized = tokenize_data(X_valid)

# OCEAN: {0:O, 1:C, 2:E, 3:A, 4:N }
def get_label(y, ocean):
  label = []
  for data in y:
    label.append(data[ocean])
  return label

def clean_X_y(X, label):
  df = pd.DataFrame(X)
  df['label'] = label
  df.drop_duplicates(inplace = True)
  print(df)
  return df[df.columns[0]], df[df.columns[1]]

ocean = 2
label_train = get_label(y_train, ocean)
label_valid = get_label(y_valid, ocean)
X_train_cleaned, y_train_cleaned = clean_X_y(X_train_tokenized, label_train)
X_valid_cleaned, y_valid_cleaned = clean_X_y(X_valid_tokenized, label_valid)

# load stopword
def load_stopwords(stopwords_file):
  stopwords = []
  with codecs.open(stopwords_file, 'r', encoding='utf-8', errors='ignore') as fp:
      stopwords = fp.read().split('\n')
  return stopwords

stopwords_file     = dataset_dir + "/stopwords-master/cn_stopwords.txt"
stop_words = load_stopwords(stopwords_file)

vectorizer = CountVectorizer(stop_words=stop_words)
X_features_train = vectorizer.fit_transform(X_train_cleaned)

# use X_features_train.toarray(), np.array(y_train_cleaned)

sum(y_train_cleaned)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

svm_model = SVC(kernel='poly', C=1.4, gamma='scale', class_weight='balanced')
svm_model.fit(X_features_train, y_train_cleaned)

X_valid_features =  vectorizer.transform(X_valid_cleaned)

# Evaluate SVM model
y_pred = svm_model.predict(X_valid_features)
accuracy = accuracy_score(y_valid_cleaned, y_pred)
print('Accuracy:', accuracy)
for label in [0, 1]:
    print('Precision for label {} = {}'.format(label, metrics.precision_score(y_pred, y_valid_cleaned, pos_label=label)))
    print('Recall    for label {} = {}'.format(label, metrics.recall_score(y_pred,    y_valid_cleaned, pos_label=label)))